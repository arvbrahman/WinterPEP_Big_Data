{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jjHwNXHIYKXb",
        "CRFWRRLvYpFS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BalgNH57lX0j",
        "outputId": "e94fdd9f-df0c-41de-b670-9715864991e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "#Performance Integrated Platform\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Big Data Tools Overview\").getOrCreate()\n",
        "\n",
        "#Creating a dataframe\n",
        "data = [(\"Alice\",29), (\"Bob\",34), (\"Catherine\",27)]\n",
        "columns = [\"Name\",\"Age\"]\n",
        "df = spark.createDataFrame(data,schema=columns)\n",
        "\n",
        "#Printing dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1zu-8wplyrN",
        "outputId": "b98e0585-3188-4486-d0ad-cc4fffeff454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 29|\n",
            "|      Bob| 34|\n",
            "|Catherine| 27|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HDFS (Hadoop Component)"
      ],
      "metadata": {
        "id": "jjHwNXHIYKXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HDFS\n",
        "# Developing the function for HDFS\n",
        "def split_file(file_content,chunk_size):\n",
        "      chunks = [file_content[i:i+chunk_size]\n",
        "                for i in range(0,len(file_content),chunk_size) ]\n",
        "      return chunks\n",
        "\n",
        "\n",
        "# Dividing into chunks for a sample data\n",
        "file_content = \"jbuhuhjnj n hjjhi iuihijhkj jih ijhjheunknhgsen hinebyiuonms uijnb eg\"\n",
        "chunk_size = 5\n",
        "chunks = split_file(file_content,chunk_size)\n",
        "\n",
        "\n",
        "# Displaying the total chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "  print(f\"Node {i+1}: {chunk}\")"
      ],
      "metadata": {
        "id": "kTtrcvCQmDk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a2d1269a-160e-48b3-d298-f2b75c7a75cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 1: jbuhu\n",
            "Node 2: hjnj \n",
            "Node 3: n hjj\n",
            "Node 4: hi iu\n",
            "Node 5: ihijh\n",
            "Node 6: kj ji\n",
            "Node 7: h ijh\n",
            "Node 8: jheun\n",
            "Node 9: knhgs\n",
            "Node 10: en hi\n",
            "Node 11: nebyi\n",
            "Node 12: uonms\n",
            "Node 13:  uijn\n",
            "Node 14: b eg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MAP REDUCE"
      ],
      "metadata": {
        "id": "CRFWRRLvYpFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library from framework\n",
        "from collections import Counter\n",
        "\n",
        "# loading the dataset\n",
        "data = [\"big data ecosystem\",\"data architecture\",\"big data\",\"big data hadoop\"]     #Already splitted data\n",
        "\n",
        "# Mapping\n",
        "mapped = []        #method\n",
        "for line in data:\n",
        "    words = line.split()\n",
        "    mapped.extend([(word,1) for word in words])\n",
        "\n",
        "# Reducing\n",
        "reduced = Counter()\n",
        "for key, value in mapped:\n",
        "    reduced[key] += value\n",
        "\n",
        "# Displaying\n",
        "for word, count in reduced.items():\n",
        "    print(f\"{word} : {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfDEk0FfYcMt",
        "outputId": "74dbdd8b-cbaa-40dc-92fb-7cc79769008a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "big : 3\n",
            "data : 4\n",
            "ecosystem : 1\n",
            "architecture : 1\n",
            "hadoop : 1\n"
          ]
        }
      ]
    }
  ]
}